{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNNs for Music Genre Classification based on MEL Spectograms\n",
    "\n",
    "### Example data from MTG-Jamendo dataset"
   ],
   "id": "605e45aa2056c08d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "from os import getcwd\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from matplotlib.pyplot import annotate\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ],
   "id": "1fed3fddcbbc85b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data from npy file\n",
    "example = np.load('assets/1354100.npy')\n",
    "example.shape"
   ],
   "id": "1bca2d2e9b8066e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.imshow(example)\n",
    "fig.show()"
   ],
   "id": "60ad51cce3422506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "example_2 = np.load('assets/1169700.npy')\n",
    "example_2.shape"
   ],
   "id": "c215f36bc5df858a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.imshow(example_2)\n",
    "fig.show()"
   ],
   "id": "bfe3c6d960ec9301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see the data is not cut into 30s segments, and for cnn we need to cut it into 30s segments. For that we need to answer a question: how was the mel spectogram obtained. I managed to dig into the code of MTG-Jamendo dataset and found the following code:\n",
    "```python\n",
    "def melspectrogram(audio,\n",
    "                   sampleRate=12000, frameSize=512, hopSize=256,\n",
    "                   window='hann', zeroPadding=0, center=True,\n",
    "                   numberBands=96, lowFrequencyBound=0, highFrequencyBound=None,\n",
    "                   weighting='linear', warpingFormula='slaneyMel',\n",
    "                   normalize='unit_tri'):\n",
    "\n",
    "    if highFrequencyBound is None:\n",
    "        highFrequencyBound = sampleRate/2\n",
    "\n",
    "    windowing = Windowing(type=window, normalized=False, zeroPadding=zeroPadding)\n",
    "    spectrum = Spectrum()\n",
    "    melbands = MelBands(numberBands=numberBands,\n",
    "                        sampleRate=sampleRate,\n",
    "                        lowFrequencyBound=lowFrequencyBound,\n",
    "                        highFrequencyBound=highFrequencyBound,\n",
    "                        inputSize=(frameSize+zeroPadding)//2+1,\n",
    "                        weighting=weighting,\n",
    "                        normalize=normalize,\n",
    "                        warpingFormula=warpingFormula,\n",
    "                        type='power')\n",
    "    amp2db = UnaryOperator(type='lin2db', scale=2)\n",
    "\n",
    "    pool = essentia.Pool()\n",
    "    for frame in FrameGenerator(audio,\n",
    "                                frameSize=frameSize, hopSize=hopSize,\n",
    "                                startFromZero=not center):\n",
    "        pool.add('mel', amp2db(melbands(spectrum(windowing(frame)))))\n",
    "\n",
    "    return pool['mel'].T\n",
    "```\n",
    "Let's see if the parameters are the same as the ones used to generate the mel spectogram."
   ],
   "id": "b16d443dc42d7a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters from the melspectrogram function\n",
    "sample_rate = 12000  # in Hz\n",
    "frame_size = 512     # in samples\n",
    "hop_size = 256       # in samples\n",
    "\n",
    "# Frame duration in seconds\n",
    "frame_duration_seconds = hop_size / sample_rate\n",
    "\n",
    "# Example mapping of time frames (N) to duration in seconds\n",
    "N_values = [example.shape[1], example_2.shape[1], 1376]  # Example N values (number of frames)\n",
    "durations = [round(N * frame_duration_seconds, 1) for N in N_values]\n",
    "\n",
    "frame_duration_seconds, list(zip(N_values, durations))"
   ],
   "id": "4dba9c3419d8c0aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Based on the data above let's create a function for generating mel spectograms with librosa\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_mel_spectrogram(audio_path, sample_rate=12000, frame_size=512, hop_size=256):\n",
    "    \"\"\"\n",
    "    Generate a mel spectrogram from an audio file and save it as a npy file\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the input audio file.\n",
    "        output_path (str): Path to save the generated mel spectrogram npy file.\n",
    "\n",
    "    Returns: mel spectogram as np array\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "\n",
    "    # Short-time Fourier transform (STFT)\n",
    "    D = librosa.stft(y, n_fft=frame_size, hop_length=hop_size)\n",
    "\n",
    "    # gathering the absolute values for all values in our audio_stft\n",
    "    magnitude = np.abs(D)\n",
    "\n",
    "    # Converting the amplitude to decibels\n",
    "    log_power = librosa.amplitude_to_db(magnitude, ref=np.max)\n",
    "    \n",
    "    # Create and save the mel spectrogram as a npy file to the output path\n",
    "    \n",
    "\n",
    "\n"
   ],
   "id": "1569538cfc6f8aec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check the duration of the tracks in the dataset:",
   "id": "2be7594b27f95cc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mtg_genre_tags_filepath = 'assets/autotagging_genre.tsv'\n",
    "columns = ['track_id', 'artist_id', 'album_id', 'path', 'duration', 'tag1', 'tag2', 'tag3', 'tag4', 'tag5', 'tag6',\n",
    "           'tag7', 'tag8', 'tag9']\n",
    "\n",
    "data = pd.read_csv(mtg_genre_tags_filepath, sep='\\t', names=columns, engine='python')"
   ],
   "id": "49667cc92e8c8606",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[data['path'].isin(['00/1354100.mp3', '00/1169700.mp3'])][['path', 'duration']]",
   "id": "1ad7b8c881197e92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The calculated duration is the same as in the dataset, so we can proceed with cutting the mel spectograms into 30-second segments.\n",
    "\n",
    "\n",
    "### Cutting Mel Spectograms to 30s Segments and savind to spectograms_data folder\n",
    "\n",
    "\n",
    "> 1376 frames correspond to 29.4 seconds of audio, given the frame size of 512 samples and hop size of 256 samples and is easily dividable by 2."
   ],
   "id": "d67cbc4f6a1b13e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cut_mel_spectrogram_to_30s(input_file_name, output_path, cut_length=1376): # 1376 is shorter than 30 second but easily dividable by 2\n",
    "    \"\"\"\n",
    "    Cut a mel spectrogram to 30 seconds, selecting the middle portion of the spectrogram.\n",
    "\n",
    "    Args:\n",
    "        input_file_name (str): Path to the input mel spectrogram file in .npy format.\n",
    "        output_path (str): Path to save the cut mel spectrogram.\n",
    "        cut_length (int): Number of frames to keep, default is 1406 frames (30 seconds).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the mel spectrogram from the npy file\n",
    "        mel_spectrogram = np.load(input_file_name)\n",
    "\n",
    "        # Ensure the spectrogram has enough frames\n",
    "        num_frames = mel_spectrogram.shape[1]\n",
    "        if num_frames < cut_length:\n",
    "            raise ValueError(f\"The spectrogram is shorter than the required {cut_length} frames.\")\n",
    "        else:\n",
    "\n",
    "            # Calculate the start and end frames to cut the middle part\n",
    "            start_frame = (num_frames - cut_length) // 2\n",
    "            end_frame = start_frame + cut_length\n",
    "\n",
    "            # Slice the spectrogram to get the 30-second segment\n",
    "            mel_spectrogram_cut = mel_spectrogram[:, start_frame:end_frame]\n",
    "\n",
    "            # Save the cut mel spectrogram to spectograms_data folder with the same name\n",
    "            np.save(output_path, mel_spectrogram_cut)\n",
    "\n",
    "    except ValueError:\n",
    "        print(f\"The spectrogram for {input_file_name} is shorter than the required {cut_length} frames.\")\n"
   ],
   "id": "16749b799383fc06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Download tar file from mtg-jamendo dataset (if not in the temp_file folder) and extract it folder by folder (from 00 to 99 tar files f.ex. raw_30s_melspecs-00.tar) with deleting tar file, then run the function to cut all mel spectograms that are in the assets/genres_one_genre.csv file in the assets folder (search by path = folder + track name). Then delete extracted folders. Repeat for all tar files.",
   "id": "c875ac7050e5e4d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CHUNK_SIZE = 512 * 1024  # 512 KB\n",
    "BASE_URL = \"https://cdn.freesound.org/mtg-jamendo/raw_30s/melspecs/\"\n",
    "\n",
    "# Function to download tar file from the MTG-Jamendo dataset, track with tqdm\n",
    "def download_tar_file(tar_file_name, tar_folder_path):\n",
    "    \"\"\"\n",
    "    Download a tar file from the MTG-Jamendo dataset.\n",
    "\n",
    "    Args:\n",
    "        tar_file_name (str): Name of the tar file to download.\n",
    "        tar_folder_path (str): Path to save the downloaded tar file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if the tar file already exists\n",
    "    tar_path = os.path.join(tar_folder_path, tar_file_name)\n",
    "    if os.path.exists(tar_path):\n",
    "        print(f\"The tar file {tar_file_name} already exists.\")\n",
    "        return\n",
    "\n",
    "    # Download the tar file from the MTG-Jamendo dataset\n",
    "    url = BASE_URL + tar_file_name\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(tar_path, 'wb') as f:\n",
    "            for chunk in tqdm.tqdm(r.iter_content(chunk_size=CHUNK_SIZE)):\n",
    "                f.write(chunk)\n",
    "\n",
    "def extract_tar_files(tar_folder_path, extract_folder_path):\n",
    "    \"\"\"\n",
    "    Extract all tar files in the given folder to the extract folder.\n",
    "\n",
    "    Args:\n",
    "        tar_folder_path (str): Path to the folder containing tar files.\n",
    "        extract_folder_path (str): Path to the folder to extract the tar files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # List all files in the tar folder\n",
    "    tar_files = [f for f in os.listdir(tar_folder_path) if f.endswith('.tar')]\n",
    "\n",
    "    # Extract each tar file to the extract folder\n",
    "    for tar_file in tar_files:\n",
    "        tar_path = os.path.join(tar_folder_path, tar_file)\n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            tar.extractall(extract_folder_path)\n",
    "        # Delete the tar file after extraction\n",
    "        os.remove(tar_path)\n",
    "\n",
    "def cut_mel_spectrograms_from_folder(folder_path, output_folder):\n",
    "    \"\"\"\n",
    "    Cut mel spectrograms in the given folder to 30-second segments.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing mel spectrogram files.\n",
    "        output_folder (str): Path to save the cut mel spectrograms.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Print folders in the path\n",
    "    mtg_jamendo_folder_path = os.listdir(folder_path)[0]\n",
    "    # Take all files in the mtg_jamendo_folder_path without ending .npy\n",
    "    files = [f for f in os.listdir(os.path.join(folder_path, mtg_jamendo_folder_path)) if f.endswith('.npy')]\n",
    "    file_names = [f'{mtg_jamendo_folder_path}/{f.replace('.npy','')}.mp3' for f in files] # This is the path in the autotagging_genre_filtered.csv file\n",
    "\n",
    "\n",
    "    # For each file in the folder if file name is in the assets/genres_one_genre.csv file (path) cut the mel spectogram to 30s. Take the genre from the same file and save it to spectograms_data folder to the folder with the genre name. Function to cut the mel spectogram to 30s is cut_mel_spectrogram_to_30s\n",
    "\n",
    "    data = pd.read_csv('assets/genres_one_genre.csv')\n",
    "\n",
    "    for file, file_name in zip(files, file_names):\n",
    "         if file_name in data['path'].values:\n",
    "            genre = data[data['path'] == file_name]['genre'].values[0]\n",
    "            output_path = os.path.join(output_folder, genre)\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "            input_file = os.path.join(folder_path, mtg_jamendo_folder_path, file)\n",
    "            cut_mel_spectrogram_to_30s(input_file, os.path.join(output_path, f'{mtg_jamendo_folder_path}_{file}'))\n",
    "\n",
    "\n",
    "def delete_extracted_folders(extract_folder_path):\n",
    "    \"\"\"\n",
    "    Delete all folders in the given path.\n",
    "\n",
    "    Args:\n",
    "        extract_folder_path (str): Path to the folder containing folders to delete.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # List all files in the folder\n",
    "    mtg_jamendo_folder_path = os.listdir(extract_folder_path)[0]\n",
    "\n",
    "    # Delete the folder with its contents\n",
    "    folder_path = os.path.join(extract_folder_path, mtg_jamendo_folder_path)\n",
    "    for f in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            os.rmdir(file_path)\n",
    "    # Delete the empty folder\n",
    "    os.rmdir(folder_path)\n",
    "    print(\"Deleted FOLDER_PATH: \", folder_path)\n"
   ],
   "id": "b2da72f2a990706c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download and extract tar files\n",
    "tar_folder_path = 'temp_files'\n",
    "extract_folder_path = 'temp_extracted'"
   ],
   "id": "2231b3fdc8e0b51c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# download_tar_file('raw_30s_melspecs-05.tar', tar_folder_path)\n",
    "# extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "# cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "# delete_extracted_folders(extract_folder_path)"
   ],
   "id": "a19edbbe1f9b5880",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the lenght of example file in the spectograms_data folder\n",
    "example_cut = np.load('spectograms_data/pop/00_268400.npy')\n",
    "example_cut.shape"
   ],
   "id": "d2424499bd31297d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# viusalize the cut mel spectogram\n",
    "fig = px.imshow(example_cut)\n",
    "fig.show()"
   ],
   "id": "5d632299dc0d6506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check how long does it take to download 00 tar file, extract it and cut the mel spectograms\n",
    "start_time = time.time()\n",
    "download_tar_file('raw_30s_melspecs-00.tar', tar_folder_path)\n",
    "extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "delete_extracted_folders(extract_folder_path)\n",
    "\n",
    "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")"
   ],
   "id": "829c6ac4b681f902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Do the same for all targ files from 02 to 99 in 4 separate cells in 4 batches\n",
    "\n",
    "# Batch 1 - tar files 01-24\n",
    "\n",
    "for i in range(1, 5):\n",
    "    tar_file_name = f'raw_30s_melspecs-{i:02d}.tar'\n",
    "    download_tar_file(tar_file_name, tar_folder_path)\n",
    "    extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "    cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "    delete_extracted_folders(extract_folder_path)"
   ],
   "id": "9dfdbe19229e2cc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Do the same for all targ files from 02 to 99 in 4 separate cells in 4 batches\n",
    "\n",
    "# Batch 1 - tar files 01-24\n",
    "\n",
    "for i in range(6, 15):\n",
    "    tar_file_name = f'raw_30s_melspecs-{i:02d}.tar'\n",
    "    download_tar_file(tar_file_name, tar_folder_path)\n",
    "    extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "    cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "    delete_extracted_folders(extract_folder_path)"
   ],
   "id": "59ae25b6f0d42c4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Do the same for all targ files from 02 to 99 in 4 separate cells in 4 batches\n",
    "\n",
    "# Batch 1 - tar files 01-24\n",
    "# TODO: need to cut mel spectograms from files 00-16 to 1376 frames later\n",
    "for i in range(17, 25):\n",
    "    tar_file_name = f'raw_30s_melspecs-{i:02d}.tar'\n",
    "    download_tar_file(tar_file_name, tar_folder_path)\n",
    "    extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "    cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "    delete_extracted_folders(extract_folder_path)"
   ],
   "id": "a85f048f529a8001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Batch 2 - tar files 25-49\n",
    "\n",
    "for i in range(25, 50):\n",
    "    tar_file_name = f'raw_30s_melspecs-{i:02d}.tar'\n",
    "    download_tar_file(tar_file_name, tar_folder_path)\n",
    "    extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "    cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "    delete_extracted_folders(extract_folder_path)"
   ],
   "id": "db1e3aeb643d4a8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Batch 3 - tar files 50-74\n",
    "\n",
    "for i in range(71, 75):\n",
    "    tar_file_name = f'raw_30s_melspecs-{i:02d}.tar'\n",
    "    download_tar_file(tar_file_name, tar_folder_path)\n",
    "    extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "    cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "    delete_extracted_folders(extract_folder_path)\n"
   ],
   "id": "46d9a9c32fc9b4a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Batch 4 - tar files 75-99\n",
    "\n",
    "for i in range(75, 100):\n",
    "    tar_file_name = f'raw_30s_melspecs-{i:02d}.tar'\n",
    "    download_tar_file(tar_file_name, tar_folder_path)\n",
    "    extract_tar_files(tar_folder_path, extract_folder_path)\n",
    "    cut_mel_spectrograms_from_folder(extract_folder_path, 'spectograms_data')\n",
    "    delete_extracted_folders(extract_folder_path)"
   ],
   "id": "9100c3dd3366aa5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count number of files in each subfolder in spectograms_data\n",
    "count_dictionary = dict()\n",
    "genres = ['pop', 'metal', 'rock', 'hiphop', 'rap', 'indie', 'jazz', 'country']\n",
    "\n",
    "for genre in genres:\n",
    "    genre_folder = os.path.join('spectograms_data', genre)\n",
    "    count = 0\n",
    "    for f in os.listdir(genre_folder):\n",
    "        count += 1\n",
    "    count_dictionary[genre] = count"
   ],
   "id": "93dc7b190a99d73a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "count_dictionary",
   "id": "59d21c1f5c8f6e87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sum(count_dictionary.values())",
   "id": "217cbc4c74bf27b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cut all of the mel spectograms in spectograms_data to 1376 frames (if they are longer)\n",
    "\n",
    "def cut_all_mel_spectrograms_to_1376_frames(spectograms_data_path, cut_length=1376):\n",
    "    \"\"\"\n",
    "    Cut all mel spectrograms in the given folder to 1376 frames.\n",
    "\n",
    "    Args:\n",
    "        spectograms_data_path (str): Path to the folder containing mel spectrogram files.\n",
    "        cut_length (int): Number of frames to keep, default is 1376 frames.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    genres = [d for d in os.listdir(spectograms_data_path) if os.path.isdir(os.path.join(spectograms_data_path, d))]\n",
    "    for genre in genres:\n",
    "        genre_folder = os.path.join(spectograms_data_path, genre)\n",
    "        for f in os.listdir(genre_folder):\n",
    "            input_file = os.path.join(genre_folder, f)\n",
    "            cut_mel_spectrogram_to_30s(input_file, input_file, cut_length=cut_length)\n",
    "\n",
    "# Cut all mel spectograms to 1376 frames\n",
    "cut_all_mel_spectrograms_to_1376_frames('spectograms_data')"
   ],
   "id": "176caf3570920927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the shape of random mel spectogram from each genre\n",
    "\n",
    "genres = [d for d in os.listdir('spectograms_data') if os.path.isdir(os.path.join('spectograms_data', d))]\n",
    "for genre in genres:\n",
    "    genre_folder = os.path.join('spectograms_data', genre)\n",
    "    files = [f for f in os.listdir(genre_folder) if f.endswith('.npy')]\n",
    "    random_file = random.choice(files)\n",
    "    random_spectrogram = np.load(os.path.join(genre_folder, random_file))\n",
    "    print(f\"Genre: {genre}, File: {random_file}, Shape: {random_spectrogram.shape}\")"
   ],
   "id": "cf08debe597183e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize randomly selected mel spectograms from each genre\n",
    "spectograms_data_path = 'spectograms_data'\n",
    "\n",
    "# Function to visualize a randomly selected spectrogram from each genre\n",
    "def visualize_random_spectrograms(spectograms_data_path):\n",
    "    genres = [d for d in os.listdir(spectograms_data_path) if os.path.isdir(os.path.join(spectograms_data_path, d))]\n",
    "    for genre in genres:\n",
    "        genre_folder = os.path.join(spectograms_data_path, genre)\n",
    "        print(genre_folder)\n",
    "        if os.path.exists(genre_folder):\n",
    "            files = [f for f in os.listdir(genre_folder) if f.endswith('.npy')]\n",
    "            if files:\n",
    "                random_file = random.choice(files)\n",
    "                spectrogram = np.load(os.path.join(genre_folder, random_file))\n",
    "                fig = px.imshow(spectrogram, title=f'Random {genre} Spectrogram')\n",
    "                fig.show()\n",
    "            else:\n",
    "                print(f\"No files found in {genre_folder}\")\n",
    "        else:\n",
    "            print(f\"Genre folder {genre_folder} does not exist\")\n",
    "\n",
    "# Visualize randomly selected spectrograms\n",
    "visualize_random_spectrograms(spectograms_data_path)"
   ],
   "id": "31bbb36168d815cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the cut mel spectogram\n",
    "example_cut = np.load('spectograms_data/country/24_1128824.npy')\n",
    "# Check the shape of the mel spectogram\n",
    "example_cut.shape"
   ],
   "id": "879f3cf6cbb939bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check how min and max values of the mel spectogram\n",
    "example_cut.min(), example_cut.max()\n",
    "# This is scale in dB, so the values are negative"
   ],
   "id": "74fa588fd7680056",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build a CNN model for music genre classification using the MEL spectrogram\n",
    "# First build a class MelSpectogramDataset to load data for torch DataLoader\n",
    "\n",
    "class MelSpectogramDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, max_samples_for_class_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the MelSpectogramDataset class.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to the folder containing the mel spectrogram files.\n",
    "            transform (callable): Optional transform to be applied to the mel spectrogram.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.max_samples_for_class_dict = max_samples_for_class_dict\n",
    "        self.transform = transform\n",
    "        self.genres = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for i, genre in enumerate(self.genres):\n",
    "            genre_folder = os.path.join(data_path, genre)\n",
    "            files = [f for f in os.listdir(genre_folder) if f.endswith('.npy')]\n",
    "\n",
    "            self.file_paths.extend([os.path.join(genre_folder, f) for f in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mel_spectrogram = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram, label\n",
    "\n",
    "\n"
   ],
   "id": "d7d84262e578d704",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MelSpectogramDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, max_samples_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the MelSpectogramDataset class.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to the folder containing the mel spectrogram files.\n",
    "            transform (callable): Optional transform to be applied to the mel spectrogram.\n",
    "            max_samples_dict (dict): Dictionary specifying the maximum number of samples per genre. \n",
    "                                     For example: {'rock': 1000, 'pop': 600}.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.max_samples_dict = max_samples_dict\n",
    "        self.genres = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i, genre in enumerate(self.genres):\n",
    "            genre_folder = os.path.join(data_path, genre)\n",
    "            files = [f for f in os.listdir(genre_folder) if f.endswith('.npy')]\n",
    "\n",
    "            # Check if this genre has a max_samples limit\n",
    "            if max_samples_dict and genre in max_samples_dict:\n",
    "                max_samples = max_samples_dict[genre]\n",
    "                files = np.random.choice(files, size=min(max_samples, len(files)), replace=False).tolist()\n",
    "\n",
    "            self.file_paths.extend([os.path.join(genre_folder, f) for f in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mel_spectrogram = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram, label\n"
   ],
   "id": "452b585abd95f030",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build class CNNModel for CNN model for music genre classification\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_filters):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(num_filters * 8 * 6 * 86, 256)\n",
    "        self.linear2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.Flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ],
   "id": "b7674e77d8b27f8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Render model summary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "model = CNNModel(num_classes=8, num_filters=16)\n",
    "summary(model, (1, 96, 1376))"
   ],
   "id": "85685569538020da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = CNNModel(num_classes=8, num_filters=40)\n",
    "summary(model, (1, 96, 1376))"
   ],
   "id": "9f729a514adc2847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "3fa17d408a88939b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example use for MelSpectogramDataset and DataLoader\n",
    "# Create a MelSpectogramDataset instance\n",
    "mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "# Create a DataLoader instance\n",
    "mel_loader = DataLoader(mel_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for i, (mel_spectrogram, label) in enumerate(mel_loader):\n",
    "    print(f\"Batch {i + 1}:\")\n",
    "    print(\"Shape of mel_spectrogram:\", mel_spectrogram.shape)\n",
    "    print(\"Labels:\", label)\n",
    "    break"
   ],
   "id": "e0744bf3e88bdb11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print one instance of the mel spectogram dataset (with the file name)\n",
    "\n",
    "mel_spectrogram, label = mel_dataset[1]\n",
    "print(\"Shape of mel_spectrogram:\", mel_spectrogram.shape)\n",
    "print(\"Label:\", label)"
   ],
   "id": "ca155e920e3f5b64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mel_spectrogram, label = mel_dataset[12000]\n",
    "print(mel_dataset.file_paths[12000])\n",
    "print(mel_dataset.labels[12000])\n",
    "print(mel_dataset.genres[label])"
   ],
   "id": "ba0ffab23d29e223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ca90e73d2f03b845",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a6da8a799e35596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print genres with corresponding label\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"Label: {label}, Genre: {mel_dataset.genres[label]}\")"
   ],
   "id": "99aa3ace7aa4e85a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_probs, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate AUC, precision, recall, and F1 score for multiclass classification.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred_probs (array-like): Predicted probabilities or logits.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing AUC, precision, recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to class predictions\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_pred_probs, multi_class=\"ovr\", average=\"macro\"),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }\n",
    "    return metrics\n"
   ],
   "id": "5c39693d7c7bb3c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def per_class_metrics(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Prints classification metrics for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "    \"\"\"\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        zero_division=1,\n",
    "        target_names=[f\"Class {i}\" for i in range(num_classes)]\n",
    "    )\n",
    "    print(report)"
   ],
   "id": "5b8025648afff532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define class EarlyStopping for early stopping in the training loop\n",
    "import copy\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_model is None:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, reset counter to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement, counter {self.counter}\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Stopping early, counter {self.counter}\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ],
   "id": "c2847a5e711c870f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create instance of MelSpectogramDataset with undersampling for pop, rock and hiphop genres max 1000 samples\n",
    "max_samples_dict = {\n",
    "    \"pop\": 1000,\n",
    "    \"rock\": 1000,\n",
    "    \"hiphop\": 1000\n",
    "}\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = MelSpectogramDataset(\n",
    "    data_path=\"spectograms_data\",\n",
    "    transform=None,\n",
    "    max_samples_dict=max_samples_dict\n",
    ")\n",
    "\n",
    "# Use DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over batches\n",
    "for mel_spectrogram, label in dataloader:\n",
    "    print(mel_spectrogram.shape, label)"
   ],
   "id": "48165afedf723a25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count the number of samples for each genre\n",
    "\n",
    "genre_counts = {genre: 0 for genre in mel_dataset.genres}\n",
    "for label in mel_dataset.labels:\n",
    "    genre_counts[mel_dataset.genres[label]] += 1\n",
    "\n",
    "genre_counts"
   ],
   "id": "83dc07f78f2c8a4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mel_dataset.genres",
   "id": "33b0d8743b9ec76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e5492d9fe12f78c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# StratifiedKFold cross-validation for CNN model (10 splits)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the number of splits for KFold cross-validation\n",
    "n_splits = 10\n",
    "\n",
    "# Create a KFold instance\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Set fixed random number of seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Make use of GPU if available or MPS (Apple) if one is available\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_built()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Iterate over the KFold splits and train the CNN model (using the MPS device if available)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(mel_dataset.file_paths, mel_dataset.labels)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "\n",
    "    # Create a CNN model instance\n",
    "    model = CNNModel(num_classes=8).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer with early stopping\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "    es = EarlyStopping()\n",
    "\n",
    "\n",
    "    # Create a DataLoader instance for training and testing\n",
    "    train_loader = DataLoader(MelSpectogramDataset(data_path='spectograms_data', transform=None),\n",
    "                              batch_size=32, sampler=torch.utils.data.SubsetRandomSampler(train_index))\n",
    "    test_loader = DataLoader(MelSpectogramDataset(data_path='spectograms_data', transform=None),\n",
    "                             batch_size=32, sampler=torch.utils.data.SubsetRandomSampler(test_index))\n",
    "\n",
    "    # Train the CNN model\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        all_labels, all_probs = [], []\n",
    "        all_labels_per_class, all_probs_per_class = [], []\n",
    "        test_progress = tqdm.tqdm(test_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "                all_labels_per_class.extend(label.cpu().numpy())\n",
    "                all_probs_per_class.extend(output.cpu().numpy())\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted == label).sum().item()\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(output, label)\n",
    "                test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "\n",
    "                # Calculate auc, accuracy, precision, recall, f1 score\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        all_labels_per_class = np.array(all_labels_per_class)\n",
    "        all_probs_per_class = np.array(all_probs_per_class)\n",
    "        y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "        # Per-class metrics\n",
    "        print(f\"Fold {i + 1}, Epoch {epoch + 1}:\")\n",
    "        print(\"PER CLASS METRICS\")\n",
    "        per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "        print(\"OVERALL METRICS\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "        print(f\"Epoch {epoch + 1}: Accuracy: {accuracy:.2f}% | Metrics: {metrics}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if es(model, loss.item()):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"models/model_fold_{i + 1}.pt\")"
   ],
   "id": "e51b9643491de17d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bba03c7d822ce03b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for the given true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True class labels.\n",
    "        y_pred (array-like): Predicted class labels.\n",
    "        average (str): Type of averaging to perform ('micro', 'macro', 'weighted', or None).\n",
    "\n",
    "    Returns:\n",
    "        float: The F1 score.\n",
    "    \"\"\"\n",
    "    return f1_score(y_true, y_pred, average=average)"
   ],
   "id": "6e893129a1dae29e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 8, 64, step=8)\n",
    "    \n",
    "    # Create a MelSpectogramDataset instance\n",
    "    mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(mel_dataset)),\n",
    "        test_size=0.1,\n",
    "        stratify=mel_dataset.labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        mel_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        mel_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    )\n",
    "    \n",
    "    # Set fixed random number of seed\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Make use of GPU if available or MPS (Apple) if one is available\n",
    "    device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_built()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Define the model with dynamic num_filters\n",
    "    model = CNNModel(num_classes=8, num_filters=num_filters).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        all_labels, all_probs = [], []\n",
    "        test_progress = tqdm.tqdm(test_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = calculate_metrics(all_labels, all_probs, num_classes=8)[\"F1 Score\"]\n",
    "        \n",
    "        trial.report(f1, epoch)\n",
    "\n",
    "        # Prune if needed based on the reported F1 score\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Optionally, print the F1 score for each epoch\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the F1 score after the training loop\n",
    "    return f1"
   ],
   "id": "9de0e9986ba7c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna study\n",
    "study_name = \"cnn_optimization_v1\"\n",
    "# Create a file to save study name and my comments on used dataset and hyperparameters\n",
    "study_comment = \"Imbalanced full dataset. Optimizing learning rate, batch size, and number of filters for the CNN model. No cross-validation used. 90-10 split for training and testing.\"\n",
    "study_comment_file = \"study_comment.csv\"\n",
    "\n",
    "# Save name and comment to a CSV file\n",
    "with open(study_comment_file, \"w\") as f:\n",
    "    f.write(f\"Study Name,Study Comment\\n\")\n",
    "    f.write(f\"{study_name},{study_comment}\")\n",
    "    \n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///db.sqlite3\", study_name=study_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"F1 Score: {study.best_trial.value}\")\n",
    "print(f\"Params: {study.best_trial.params}\")"
   ],
   "id": "8df5ac753316f1ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c38a3d4269e718a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(5):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        all_labels, all_probs = [], []\n",
    "        all_labels_per_class, all_probs_per_class = [], []\n",
    "        test_progress = tqdm.tqdm(test_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "                all_labels_per_class.extend(label.cpu().numpy())\n",
    "                all_probs_per_class.extend(output.cpu().numpy())\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted == label).sum().item()\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(output, label)\n",
    "                test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "\n",
    "                # Calculate auc, accuracy, precision, recall, f1 score\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        all_labels_per_class = np.array(all_labels_per_class)\n",
    "        all_probs_per_class = np.array(all_probs_per_class)\n",
    "        y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "        # Per-class metrics\n",
    "        print(f\"Fold {i + 1}, Epoch {epoch + 1}:\")\n",
    "        print(\"PER CLASS METRICS\")\n",
    "        per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "        print(\"OVERALL METRICS\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(all_labels, all_probs, num_classes=8)"
   ],
   "id": "eed4e2d13e2c1201",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"models/best_model_v1.pth\")"
   ],
   "id": "7c62667505072669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\"\"\"\n",
    "Training the model on best parameters + weighted random sampler on whole DS\n",
    "\"\"\"\n",
    "# Train the model with best parameters\n",
    "# Define the model with best parameters\n",
    "best_params = study.best_params\n",
    "model = CNNModel(num_classes=8, num_filters=best_params[\"num_filters\"]).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "\n",
    "# Create dataset instance\n",
    "mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "# Split the dataset into train, validation and test with 90-5-5 split\n",
    "\n",
    "# Define the dataset sizes\n",
    "train_size = int(0.9 * len(mel_dataset))\n",
    "val_size = (len(mel_dataset) - train_size) // 2\n",
    "test_size = len(mel_dataset) - train_size - val_size \n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    mel_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42) # For reproducibility\n",
    ")\n",
    "\n",
    "# Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "# Get train dataset labels\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Get train dataset label count for each class\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = 1.0 / class_counts\n",
    "weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ],
   "id": "5aab77903348b74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(class_counts)\n",
    "labels = list(range(8))\n",
    "for i in labels:\n",
    "    print(f\"Class {i}, Genre {train_dataset.dataset.genres[i]} :{class_counts[i]} samples\")"
   ],
   "id": "c0dce6b710cceacb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plain training loop without early stopping\n",
    "num_epochs = 6\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in train_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item()\n",
    "        train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "    # Evaluate the CNN model on the validation set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_spectrogram, label in test_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "            all_probs.append(probabilities)\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "            all_labels_per_class.extend(label.cpu().numpy())\n",
    "            all_probs_per_class.extend(output.cpu().numpy())\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy: {accuracy:.2f}% | Metrics: {metrics}\")"
   ],
   "id": "fb081bb291830df7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save this model\n",
    "torch.save(model.state_dict(), \"models/best_model_v2_weighted.pth\")"
   ],
   "id": "21969c5d845c994b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "# Load the model and evaluate it on the test set\n",
    "\n",
    "# Create a CNN model instance\n",
    "test_model = CNNModel(num_classes=8, num_filters=best_params[\"num_filters\"]).to(device)\n",
    "\n",
    "# Load the model weights\n",
    "test_model.load_state_dict(torch.load(\"models/best_model_v2_weighted.pth\"))\n",
    "\n",
    "# Create a DataLoader instance for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "        output = test_model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        all_labels_per_class.extend(label.cpu().numpy())\n",
    "        all_probs_per_class.extend(output.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "        test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "        \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "    \n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "    "
   ],
   "id": "48410d74727067f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the confusion matrix with annotations on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, y_pred)\n",
    "\n",
    "# Plot the confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()"
   ],
   "id": "f1f0f3227a7124b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now plot the normalized confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_norm, cmap=\"Blues\", annot=True, fmt=\".2f\", xticklabels=mel_dataset.genres, yticklabels=mel_dataset.genres)\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ],
   "id": "5531002479bf24e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pop and hiphop are overwhealming rest of the classes because of the dataset imbalance\n",
    "# Let's try to undersample the dataset genres: pop and hiphop and train again.\n",
    "# Change the max_samples_dict to undersample pop and hiphop genres in the objective and train the model again.\n",
    "\n",
    "\n",
    "import optuna \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 8, 64, step=8)\n",
    "    \n",
    "    # Create a MelSpectogramDataset instance\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    Training the model with stratified split, undersample pop and rock, apply weighted random sampler with Optuna.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create dataset instance\n",
    "    \n",
    "    # max_samples_dict to undersample pop and rock genres\n",
    "    max_samples_dict = {\n",
    "        \"pop\": 2000,\n",
    "        \"rock\": 2000,\n",
    "    }\n",
    "    \n",
    "    mel_dataset = MelSpectogramDataset(data_path='spectograms_data', max_samples_dict=max_samples_dict)\n",
    "    \n",
    "    # Split the dataset into train, validation and test with 90-5-5 split\n",
    "\n",
    "    \n",
    "    # Access file paths and labels\n",
    "    file_paths = mel_dataset.file_paths\n",
    "    labels = mel_dataset.labels\n",
    "    \n",
    "    # Split dataset using stratification\n",
    "    train_file_paths, temp_file_paths, train_labels, temp_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    val_file_paths, test_file_paths, val_labels, test_labels = train_test_split(\n",
    "        temp_file_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    # Create Datasets for each split\n",
    "    train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "    val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "    #test_indices = [file_paths.index(path) for path in test_file_paths]\n",
    "    \n",
    "    train_dataset = Subset(mel_dataset, train_indices)\n",
    "    val_dataset = Subset(mel_dataset, val_indices)\n",
    "    #test_dataset = Subset(mel_dataset, test_indices)\n",
    "    \n",
    "    # Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "    # Get train dataset labels\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "    \n",
    "    # Get train dataset label count for each class\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    \n",
    "    # Calculate the class weights\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = [class_weights[label] for label in train_labels]\n",
    "    \n",
    "    # Create a WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "    \n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    #test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Set fixed random number of seed\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Make use of GPU if available or MPS (Apple) if one is available\n",
    "    device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_built()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Define the model with dynamic num_filters\n",
    "    model = CNNModel(num_classes=8, num_filters=num_filters).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        all_labels, all_probs = [], []\n",
    "        test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = calculate_metrics(all_labels, all_probs, num_classes=8)[\"F1 Score\"]\n",
    "        \n",
    "        trial.report(f1, epoch)\n",
    "\n",
    "        # Prune if needed based on the reported F1 score\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Optionally, print the F1 score for each epoch\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the F1 score after the training loop\n",
    "    return f1\n"
   ],
   "id": "1a87376dc8b5eaea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna study\n",
    "study_name = \"cnn_optimization_v3_undersampled_weighted_stratified\"\n",
    "# Create a file to save study name and my comments on used dataset and hyperparameters\n",
    "study_comment = \"Undersampled pop and rock to 2000 samples. Optimizing learning rate, batch size, and number of filters for the CNN model. No cross-validation used. 90-5-5 split for training and validation and testing. Stratified split used. WeightedRandomSampler used for training.\"\n",
    "study_comment_file = \"study_comment.csv\"\n",
    "\n",
    "# Write new line with study name and comment\n",
    "with open(study_comment_file, \"a\") as f:\n",
    "    f.write(f\"\\n{study_name},{study_comment}\")\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///db.sqlite3\", study_name=study_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"F1 Score: {study.best_trial.value}\")\n",
    "print(f\"Params: {study.best_trial.params}\")\n"
   ],
   "id": "aaad73cc6e1d6f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train with best parameters and evaluate the model on the test set",
   "id": "16a056349a558fdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare data\n",
    "\n",
    "max_samples_dict = {\n",
    "        \"pop\": 2000,\n",
    "        \"rock\": 2000,\n",
    "    }\n",
    "    \n",
    "mel_dataset = MelSpectogramDataset(data_path='spectograms_data', max_samples_dict=max_samples_dict)\n",
    "\n",
    "# Split the dataset into train, validation and test with 90-5-5 split\n",
    "# Access file paths and labels\n",
    "file_paths = mel_dataset.file_paths\n",
    "labels = mel_dataset.labels\n",
    "\n",
    "# Split dataset using stratification\n",
    "train_file_paths, temp_file_paths, train_labels, temp_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_file_paths, test_file_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_file_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "# Create Datasets for each split\n",
    "train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "test_indices = [file_paths.index(path) for path in test_file_paths]\n",
    "\n",
    "train_dataset = Subset(mel_dataset, train_indices)\n",
    "val_dataset = Subset(mel_dataset, val_indices)\n",
    "test_dataset = Subset(mel_dataset, test_indices)\n",
    "\n",
    "# Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "# Get train dataset labels\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Get train dataset label count for each class\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = 1.0 / class_counts\n",
    "weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "# Model\n",
    "\n",
    "# Define the model with dynamic num_filters\n",
    "model = CNNModel(num_classes=8, num_filters=best_params['num_filters']).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "# Set fixed random number of seed\n",
    "torch.manual_seed(42)\n",
    "    \n",
    "# Plain training loop without early stopping\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in train_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item()\n",
    "        train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "    # Evaluate the CNN model on the validation set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_spectrogram, label in test_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "            all_probs.append(probabilities)\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "            all_labels_per_class.extend(label.cpu().numpy())\n",
    "            all_probs_per_class.extend(output.cpu().numpy())\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy: {accuracy:.2f}% | Metrics: {metrics}\")\n",
    "# Save this model\n",
    "torch.save(model.state_dict(), \"models/best_model_v2_weighted.pth\")\n",
    "# Evaluate the model on the test set\n",
    "\n",
    "# Load the model and evaluate it on the test set\n",
    "\n",
    "# Create a CNN model instance\n",
    "test_model = CNNModel(num_classes=8, num_filters=best_params[\"num_filters\"]).to(device)\n",
    "\n",
    "# Load the model weights\n",
    "test_model.load_state_dict(torch.load(\"models/best_model_v2_weighted.pth\"))\n",
    "\n",
    "# Create a DataLoader instance for the test set\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "        output = test_model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        all_labels_per_class.extend(label.cpu().numpy())\n",
    "        all_probs_per_class.extend(output.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "        test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "\n",
    "# Plot the confusion matrix with annotations on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, y_pred)\n",
    "\n",
    "# Plot the confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()\n",
    "# Now plot the normalized confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_norm, cmap=\"Blues\", annot=True, fmt=\".2f\", xticklabels=mel_dataset.genres,\n",
    "            yticklabels=mel_dataset.genres)\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ],
   "id": "ff58f93a6a06a3a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "58002568c3afb6dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the model and evaluate it on the test set\n",
    "\n",
    "# Create a CNN model instance\n",
    "model = CNNModel(num_classes=8).to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load(\"models/model_fold_1.pt\"))\n",
    "\n",
    "# Create a DataLoader instance for the test set\n",
    "test_loader = DataLoader(MelSpectogramDataset(data_path='spectograms_data', transform=None),\n",
    "                         batch_size=32, shuffle=True)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Use Early Stopping\n"
   ],
   "id": "f9c778ee2d288f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Draw confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, y_pred)\n"
   ],
   "id": "89cf3ee3979d7b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the confusion matrix with annotations\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()\n"
   ],
   "id": "12d8e7cc7399eb2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We can see that because pop, rock and hiphop dominate the dataset, the model has a hard time classifying other genres.\n",
    "\n",
    "# Let's try to undersample the dataset genres: pop, rock and hiphop and train again."
   ],
   "id": "714dd97018188184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Undersample the dataset genres: pop, rock, and hiphop\n",
    "# Define the genres to undersample\n",
    "undersampled_genres = [\"pop\", \"rock\", \"hiphop\"]\n",
    "\n",
    "#\n"
   ],
   "id": "4a8244d079940223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train with RESNET ",
   "id": "a0a87e658d894940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "resnet = models.resnet18()\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=(1, 2), bias=True)\n",
    "resnet.maxpool = nn.MaxPool2d((2, 3), stride=(1, 2))\n",
    "resnet.fc = nn.Linear(512, 8)\n",
    "summary(resnet, (1, 96, 1376))"
   ],
   "id": "776132381e3de594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pop and hiphop are overwhealming rest of the classes because of the dataset imbalance\n",
    "# Let's try to undersample the dataset genres: pop and hiphop and train again.\n",
    "# Change the max_samples_dict to undersample pop and hiphop genres in the objective and train the model again.\n",
    "\n",
    "\n",
    "import optuna \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    \n",
    "    # Create a MelSpectogramDataset instance\n",
    "    \"\"\"\n",
    "    Training the model with stratified split, undersample pop and rock, apply weighted random sampler with Optuna.\n",
    "    \"\"\"\n",
    "    # Create dataset instance\n",
    "    \n",
    "    \n",
    "    mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "    \n",
    "    # Split the dataset into train, validation and test with 90-5-5 split\n",
    "\n",
    "    \n",
    "    # Access file paths and labels\n",
    "    file_paths = mel_dataset.file_paths\n",
    "    labels = mel_dataset.labels\n",
    "    \n",
    "    # Split dataset using stratification to train - validate\n",
    "    train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Create Datasets for each split\n",
    "    train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "    val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "    \n",
    "    train_dataset = Subset(mel_dataset, train_indices)\n",
    "    val_dataset = Subset(mel_dataset, val_indices)\n",
    "    \n",
    "    # Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "    # Get train dataset labels\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "    \n",
    "    # Get train dataset label count for each class\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    \n",
    "    # Calculate the class weights\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = [class_weights[label] for label in train_labels]\n",
    "    \n",
    "    # Create a WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "    \n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Set fixed random number of seed\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Make use of GPU if available or MPS (Apple) if one is available\n",
    "    device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_built()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Define the model with dynamic num_filters\n",
    "    model = resnet.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        all_labels, all_probs = [], []\n",
    "        test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = calculate_metrics(all_labels, all_probs, num_classes=8)[\"F1 Score\"]\n",
    "        \n",
    "        trial.report(f1, epoch)\n",
    "\n",
    "        # Prune if needed based on the reported F1 score\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Optionally, print the F1 score for each epoch\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the F1 score after the training loop\n",
    "    return f1\n"
   ],
   "id": "edbd7d6bbb2d8583",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna study\n",
    "study_name = \"resnet_optimization_v1_weighted_stratified\"\n",
    "# Create a file to save study name and my comments on used dataset and hyperparameters\n",
    "study_comment = \"Full dataset. Optimizing learning rate, batch size. No cross-validation used. 90-10 split for training and validation. Stratified split used. WeightedRandomSampler used for training.\"\n",
    "study_comment_file = \"study_comment.csv\"\n",
    "\n",
    "# Write new line with study name and comment\n",
    "with open(study_comment_file, \"a\") as f:\n",
    "    f.write(f\"\\n{study_name},{study_comment}\")\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///db.sqlite3\", study_name=study_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"F1 Score: {study.best_trial.value}\")\n",
    "print(f\"Params: {study.best_trial.params}\")\n"
   ],
   "id": "1b7025c91f64ab28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "24a44454be8ea401"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
