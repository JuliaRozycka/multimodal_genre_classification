{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "d315225fd8f28b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "class MelSpectogramDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, max_samples_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the MelSpectogramDataset class.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to the folder containing the mel spectrogram files.\n",
    "            transform (callable): Optional transform to be applied to the mel spectrogram.\n",
    "            max_samples_dict (dict): Dictionary specifying the maximum number of samples per genre. \n",
    "                                     For example: {'rock': 1000, 'pop': 600}.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.max_samples_dict = max_samples_dict\n",
    "\n",
    "        # Sort genres alphabetically\n",
    "        self.genres = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
    "\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i, genre in enumerate(self.genres):\n",
    "            genre_folder = os.path.join(data_path, genre)\n",
    "            files = [f for f in os.listdir(genre_folder) if f.endswith('.npy')]\n",
    "\n",
    "            # Check if this genre has a max_samples limit\n",
    "            if max_samples_dict and genre in max_samples_dict:\n",
    "                max_samples = max_samples_dict[genre]\n",
    "                files = np.random.choice(files, size=min(max_samples, len(files)), replace=False).tolist()\n",
    "\n",
    "            self.file_paths.extend([os.path.join(genre_folder, f) for f in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mel_spectrogram = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram, label\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred_probs, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate AUC, precision, recall, and F1 score for multiclass classification.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred_probs (array-like): Predicted probabilities or logits.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing AUC, precision, recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to class predictions\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_pred_probs, multi_class=\"ovr\", average=\"macro\"),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def per_class_metrics(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Prints classification metrics for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "    \"\"\"\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        zero_division=1,\n",
    "        target_names=[f\"Class {i}\" for i in range(num_classes)]\n",
    "    )\n",
    "    print(report)"
   ],
   "id": "d6a62863bdb91bdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pop and hiphop are overwhealming rest of the classes because of the dataset imbalance\n",
    "# Let's try to undersample the dataset genres: pop and hiphop and train again.\n",
    "# Change the max_samples_dict to undersample pop and hiphop genres in the objective and train the model again.\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Create a MelSpectogramDataset instance\n",
    "    \"\"\"\n",
    "    Training the model with stratified split, undersample pop and rock, apply weighted random sampler with Optuna.\n",
    "    \"\"\"\n",
    "    # Create dataset instance\n",
    "\n",
    "    mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "    print(mel_dataset.genres)  # Check the order of genres (alphabetical)\n",
    "\n",
    "    # Split the dataset into train, validation and test with 90-5-5 split\n",
    "    \n",
    "    # Access file paths and labels\n",
    "    file_paths = mel_dataset.file_paths\n",
    "    labels = mel_dataset.labels\n",
    "\n",
    "    # Split dataset using stratification to train - validate\n",
    "    train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Create Datasets for each split\n",
    "    train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "    val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "\n",
    "    train_dataset = Subset(mel_dataset, train_indices)\n",
    "    val_dataset = Subset(mel_dataset, val_indices)\n",
    "\n",
    "    # Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "    # Get train dataset labels\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "    # Get train dataset label count for each class\n",
    "    class_counts = np.bincount(train_labels)\n",
    "\n",
    "    # Calculate the class weights\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "    # Create a WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Set fixed random number of seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Make use of GPU if available or MPS (Apple) if one is available\n",
    "    device = (\n",
    "        \"mps\"\n",
    "        if torch.backends.mps.is_built()\n",
    "        else \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Define the model with dynamic num_filters\n",
    "    model = resnet.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        all_labels, all_probs = [], []\n",
    "        test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = calculate_metrics(all_labels, all_probs, num_classes=8)[\"F1 Score\"]\n",
    "\n",
    "        trial.report(f1, epoch)\n",
    "\n",
    "        # Prune if needed based on the reported F1 score\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Optionally, print the F1 score for each epoch\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the F1 score after the training loop\n",
    "    return f1"
   ],
   "id": "c3d4edc88facf135",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resnet = models.resnet18()\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=(1, 2), bias=True)\n",
    "resnet.maxpool = nn.MaxPool2d((2, 3), stride=(1, 2))\n",
    "resnet.fc = nn.Linear(512, 8)\n",
    "summary(resnet, (1, 96, 1024))"
   ],
   "id": "199898e3e99d1d17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resnet = models.resnet18()\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "resnet.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "resnet.fc = nn.Linear(512, 8)\n",
    "summary(resnet, (1, 96, 1024))"
   ],
   "id": "86bac8209b329d7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna study\n",
    "study_name = \"resnet_optimization_v1_weighted_stratified_alphabetical_normalized\"\n",
    "# Create a file to save study name and my comments on used dataset and hyperparameters\n",
    "study_comment = \"Full dataset. Optimizing learning rate, batch size. No cross-validation used. 90-10 split for training and validation. Stratified split used. WeightedRandomSampler used for training. Labels are sorted alphabetically.\"\n",
    "study_comment_file = \"study_comment.csv\"\n",
    "\n",
    "# Write new line with study name and comment\n",
    "with open(study_comment_file, \"a\") as f:\n",
    "    f.write(f\"\\n{study_name},{study_comment}\")\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///db.sqlite3\", study_name=study_name,\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"F1 Score: {study.best_trial.value}\")\n",
    "print(f\"Params: {study.best_trial.params}\")\n"
   ],
   "id": "ecb0ad1024c36635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_params = study.best_trial.params",
   "id": "53e39ab7b21fa52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#best_params = {'learning_rate': 1.201780968668489e-05, 'batch_size': 32}\n",
    "best_params = {'learning_rate': 1.1062787751773096e-05, 'batch_size': 64} #2nd run"
   ],
   "id": "49e3c0d613fd294e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = models.resnet18()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=(1, 2), bias=True)\n",
    "model.maxpool = nn.MaxPool2d((2, 3), stride=(1, 2))\n",
    "model.fc = nn.Linear(512, 8)\n",
    "summary(model, (1, 96, 1376))"
   ],
   "id": "e76ebff4ffc08ade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare data\n",
    "\n",
    "mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "# Split the dataset into train, validation and test with 90-5-5 split\n",
    "# Access file paths and labels\n",
    "file_paths = mel_dataset.file_paths\n",
    "labels = mel_dataset.labels\n",
    "\n",
    "# Split dataset using stratification\n",
    "train_file_paths, temp_file_paths, train_labels, temp_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_file_paths, test_file_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_file_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "# Create Datasets for each split\n",
    "train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "test_indices = [file_paths.index(path) for path in test_file_paths]\n",
    "\n",
    "train_dataset = Subset(mel_dataset, train_indices)\n",
    "val_dataset = Subset(mel_dataset, val_indices)\n",
    "test_dataset = Subset(mel_dataset, test_indices)\n",
    "\n",
    "# Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "# Get train dataset labels\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Get train dataset label count for each class\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = 1.0 / class_counts\n",
    "weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])"
   ],
   "id": "827148bf7ae0233c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get number of samples in test_loader\n",
    "\n",
    "num_samples = 0\n",
    "for mel_spectrogram, label in test_loader:\n",
    "    num_samples += mel_spectrogram.size(0)\n",
    "print(num_samples)"
   ],
   "id": "10055069a1c04b0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model\n",
    "\n",
    "# Define the model with dynamic num_filters\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "# Set fixed random number of seed\n",
    "torch.manual_seed(42)\n",
    "    \n",
    "# Plain training loop without early stopping\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in train_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item()\n",
    "        train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "    # Evaluate the CNN model on the validation set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_spectrogram, label in test_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "            all_probs.append(probabilities)\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "            all_labels_per_class.extend(label.cpu().numpy())\n",
    "            all_probs_per_class.extend(output.cpu().numpy())\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    # Per-class metrics\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "    \n",
    "# Save this model\n",
    "torch.save(model.state_dict(), \"models/resnet_model_v1_weighted_alphabetical.pt\")\n",
    "# Evaluate the model on the test set\n"
   ],
   "id": "4585cd97c29f16da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "test_model = models.resnet18()\n",
    "test_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "test_model.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "test_model.fc = nn.Linear(512, 8)\n",
    "\n",
    "test_model = test_model.to(\"mps\")\n"
   ],
   "id": "fb773048bfe9e4cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the model weights\n",
    "test_model.load_state_dict(torch.load(\"models/resnet_model_v1_weighted_alphabetical.pt\", weights_only=True))\n",
    "\n",
    "# Create a DataLoader instance for the test set\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "test_model.eval()"
   ],
   "id": "f4f005fc1faa4bc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = test_model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        all_labels_per_class.extend(label.cpu().numpy())\n",
    "        all_probs_per_class.extend(output.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "        test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "\n",
    "# Plot the confusion matrix with annotations on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, y_pred)\n",
    "\n",
    "# Plot the confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()\n",
    "# Now plot the normalized confusion matrix\n",
    "import seaborn as sns\n",
    "conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n"
   ],
   "id": "3463d94ca0a0f318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Capitalize the genre labels\n",
    "mel_dataset.genres = [genre.capitalize() for genre in mel_dataset.genres]"
   ],
   "id": "ac28910b7baa6edd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Change Hiphop to Hip-Hop\n",
    "mel_dataset.genres[1] = \"Hip-Hop\""
   ],
   "id": "8eb43801f141dd7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize the confusion matrix\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_norm, cmap=\"Blues\", annot=True, fmt=\".2f\", xticklabels=mel_dataset.genres,\n",
    "            yticklabels=mel_dataset.genres, vmax=1.0)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_normalized_resnet.png\", dpi=300)\n",
    "plt.show()"
   ],
   "id": "5beeca9a6a49530a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Get the true positive rate and false positive rate\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(8):\n",
    "    fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc[i] = roc_auc_score((all_labels == i).astype(int), all_probs[:, i])\n",
    "    \n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "for i in range(8):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{mel_dataset.genres[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "# Make background plain white\n",
    "plt.style.use('fast')\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.savefig(\"auc_roc_resnet.png\", dpi=300)\n",
    "plt.show()"
   ],
   "id": "c1ce883efdec50d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = models.resnet18()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=(1, 2), bias=True)\n",
    "model.maxpool = nn.MaxPool2d((2, 3), stride=(1, 2))\n",
    "model.fc = nn.Linear(512, 8)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/resnet_model_v1_weighted_alphabetical.pt\", weights_only=True))\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "# remove the last layer\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()\n",
    "\n",
    "# extract features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel_spectrogram, label in tqdm.tqdm(test_loader, desc=\"Extracting Features\", leave=False): # test loader size is 722 samples\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        features.append(output.cpu().numpy())\n",
    "        labels.append(label.cpu().numpy())"
   ],
   "id": "48d5d9974a6e2336",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features[0].shape",
   "id": "db87bab912b5f13a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features_concat = np.concatenate(features)\n",
    "labels_concat = np.concatenate(labels)\n",
    "print(features_concat.shape)\n",
    "print(labels_concat.shape)"
   ],
   "id": "3c362d72e2c5822d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show one example of features with label\n",
    "example_idx = 1\n",
    "\n",
    "# get example\n",
    "example_features = features_concat[example_idx]\n",
    "example_label = labels_concat[example_idx]\n",
    "\n",
    "# features\n",
    "print(f\"Features shape: {example_features.shape}\")\n",
    "\n",
    "#  label\n",
    "print(f\"Label: {example_label}\")"
   ],
   "id": "a545f66659180c0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(example_features)",
   "id": "b129872448d70e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Gatunek muzyczny (definicyjnie) identyfikuje utwory muzyczne jako należące do\n",
    "wspólnej tradycji lub zbioru konwencji. Nie jest tożsamy z formą muzyczną i stylem\n",
    "muzycznym, chociaż w praktyce terminy te są niekiedy używane zamiennie. W specy-\n",
    "fikacji kontenera danych ID3v1 można znaleźć 80 typów gatunków muzycznych, które\n",
    "z rozszerzeniem programu Winamp obejmują numerację typów gatunków aż do 191\n",
    "[3, 4]. Mimo że podział muzyki na gatunki jest w dużym stopniu subiektywny,\n",
    "istnieją kryteria percepcyjne związane z określoną instrumentacją, tempem lub struk-\n",
    "turą rytmiczną muzyki, jakie mogą posłużyć do scharakteryzowania określonego ga-\n",
    "tunku [5, 6]. Na podstawie badań subiektywnych wiadomo, że człowiek potrafi prze-\n",
    "widzieć gatunek muzyczny jedynie na podstawie 250-milisekundowego fragmentu,\n",
    "jeśli dany gatunek jest mu znany [7]. Może to sugerować, że ludzie potrafią oceniać\n",
    "gatunki muzyczne jedynie na płaszczyźnie muzycznej, bez konstruowania opisów wyż-\n",
    "szego poziomu.\n",
    "Rozpoznawanie gatunku muzycznego jest jednym z podstawowych elementów\n",
    "inteligentnych systemów tworzenia automatycznych list muzyki. Platformy stru-\n",
    "mieniowe (streamingowe) oferujące taką usługę wymagają rozwiązań umożliwiających\n",
    "jak najdokładniejsze określenie przynależności utworu do gatunku muzycznego.\n",
    "Zgodnie z aktualnym stanem wiedzy najskuteczniejszym klasyfikatorem są sztuczne\n",
    "sieci neuronowe (w tym w wersji uczenia głębokiego), dla których wejście stanowi\n",
    "spektrogram (postać 2D wektora wejściowego), współczynniki MFCC czy wektor pa-\n",
    "rametrów [8–10].\n",
    "\"\"\"\n",
    "  \n",
    "# From this text remove character -, newline and multiple spaces \n",
    "\n",
    "text = text.replace(\"\\n\", \" \").replace(\"-\", \"\").replace(\"  \", \" \")\n",
    "\n",
    "print(text)\n",
    "    \n",
    "    "
   ],
   "id": "30adac0ef1120869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare mel spectrorams of two genres: rock and jazz\n",
    "\n",
    "rock_path = \"spectograms_data/rock/00_4400.npy\"\n",
    "jazz_path = \"spectograms_data/jazz/00_1053500.npy\"\n",
    "\n",
    "# cmap magma\n",
    "\n",
    "rock_mel = np.load(rock_path)\n",
    "jazz_mel = np.load(jazz_path)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(rock_mel, cmap=\"magma\")\n",
    "ax[0].set_title(\"Rock\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(jazz_mel, cmap=\"magma\")\n",
    "ax[1].set_title(\"Jazz\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "a665304e46863995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pwd",
   "id": "a85372abc762d90e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "45ac603043b0ad5c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
