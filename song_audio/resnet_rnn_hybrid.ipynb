{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ResNet18 + RNN Hybrid Model\n",
    "\n",
    "Input should remain the same as npy file shape: input_data = (1, 96, 1376)"
   ],
   "id": "55b8aa8fe41e6947"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importing needed libraries with pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "import tqdm\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.utils.data import WeightedRandomSampler"
   ],
   "id": "5c28e3861f70bd69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DEVICE = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_built()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Create a class for hybrid model (ResNet18 + RNN), import model from Torchvision, models. Input size of the model should be the same as spectogram shape (1, 96, 1376)\n",
    "\n",
    "class ResNetRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ResNetRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(DEVICE)  # Ensure input is on the same device as the model\n",
    "        with torch.no_grad():\n",
    "            x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1, x.size(1))  # Reshape to (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(DEVICE)  # Ensure h0 is on the same device\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "id": "3d8162f99003de37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create instance of the model and print summary\n",
    "model = ResNetRNN(input_size=512, hidden_size=512, num_layers=2, num_classes=8).to(DEVICE)\n",
    "summary(model, (1, 96, 1376))"
   ],
   "id": "3ed6997baf13cef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class MelSpectogramDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, max_samples_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the MelSpectogramDataset class.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to the folder containing the mel spectrogram files.\n",
    "            transform (callable): Optional transform to be applied to the mel spectrogram.\n",
    "            max_samples_dict (dict): Dictionary specifying the maximum number of samples per genre. \n",
    "                                     For example: {'rock': 1000, 'pop': 600}.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.max_samples_dict = max_samples_dict\n",
    "        self.genres = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i, genre in enumerate(self.genres):\n",
    "            genre_folder = os.path.join(data_path, genre)\n",
    "            files = [f for f in os.listdir(genre_folder) if f.endswith('.npy')]\n",
    "\n",
    "            # Check if this genre has a max_samples limit\n",
    "            if max_samples_dict and genre in max_samples_dict:\n",
    "                max_samples = max_samples_dict[genre]\n",
    "                files = np.random.choice(files, size=min(max_samples, len(files)), replace=False).tolist()\n",
    "\n",
    "            self.file_paths.extend([os.path.join(genre_folder, f) for f in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mel_spectrogram = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram, label\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score"
   ],
   "id": "dd828dedb0bc4b42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred_probs, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate AUC, precision, recall, and F1 score for multiclass classification.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred_probs (array-like): Predicted probabilities or logits.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing AUC, precision, recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to class predictions\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_pred_probs, multi_class=\"ovr\", average=\"macro\"),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def per_class_metrics(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Prints classification metrics for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "    \"\"\"\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        zero_division=1,\n",
    "        target_names=[f\"Class {i}\" for i in range(num_classes)]\n",
    "    )\n",
    "    print(report)"
   ],
   "id": "28b240d4c6dee091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pop and hiphop are overwhealming rest of the classes because of the dataset imbalance\n",
    "# Let's try to undersample the dataset genres: pop and hiphop and train again.\n",
    "# Change the max_samples_dict to undersample pop and hiphop genres in the objective and train the model again.\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    \n",
    "    # Layers of the RNN to optimize\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "\n",
    "    # Create a MelSpectogramDataset instance\n",
    "    \"\"\"\n",
    "    Training the model with stratified split, apply weighted random sampler with Optuna.\n",
    "    \"\"\"\n",
    "    # Create dataset instance\n",
    "\n",
    "    mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "    # Split the dataset into train, validation and test with 90-5-5 split\n",
    "\n",
    "    # Access file paths and labels\n",
    "    file_paths = mel_dataset.file_paths\n",
    "    labels = mel_dataset.labels\n",
    "\n",
    "    # Split dataset using stratification to train - validate\n",
    "    train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Create Datasets for each split\n",
    "    train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "    val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "\n",
    "    train_dataset = Subset(mel_dataset, train_indices)\n",
    "    val_dataset = Subset(mel_dataset, val_indices)\n",
    "\n",
    "    # Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "    # Get train dataset labels\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "    # Get train dataset label count for each class\n",
    "    class_counts = np.bincount(train_labels)\n",
    "\n",
    "    # Calculate the class weights\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "    # Create a WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Set fixed random number of seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Make use of GPU if available or MPS (Apple) if one is available\n",
    "    device = (\n",
    "        \"mps\"\n",
    "        if torch.backends.mps.is_built()\n",
    "        else \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Define the model with dynamic num_filters\n",
    "    model = ResNetRNN(input_size=512, hidden_size=512, num_layers=num_layers, num_classes=8).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 4\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        all_labels, all_probs = [], []\n",
    "        test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = calculate_metrics(all_labels, all_probs, num_classes=8)[\"F1 Score\"]\n",
    "\n",
    "        trial.report(f1, epoch)\n",
    "\n",
    "        # Prune if needed based on the reported F1 score\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Optionally, print the F1 score for each epoch\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the F1 score after the training loop\n",
    "    return f1"
   ],
   "id": "841b681234321366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna study\n",
    "study_name = \"resnet_rnn_hybrid_optimization_v1_weighted_stratified\"\n",
    "# Create a file to save study name and my comments on used dataset and hyperparameters\n",
    "study_comment = \"Full dataset. Optimizing learning rate, batch size. No cross-validation used. 90-10 split for training and validation. Stratified split used. WeightedRandomSampler used for training.\"\n",
    "study_comment_file = \"study_comment.csv\"\n",
    "\n",
    "# Write new line with study name and comment\n",
    "with open(study_comment_file, \"a\") as f:\n",
    "    f.write(f\"\\n{study_name},{study_comment}\")\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///db.sqlite3\", study_name=study_name,\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"F1 Score: {study.best_trial.value}\")\n",
    "print(f\"Params: {study.best_trial.params}\")\n"
   ],
   "id": "174464a23a1907aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_params = study.best_trial.params\n",
    "model = ResNetRNN(input_size=512, hidden_size=512, num_layers=best_params['num_layers'], num_classes=8).to(DEVICE)\n",
    "# Prepare data\n",
    "\n",
    "mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "# Split the dataset into train, validation and test with 90-5-5 split\n",
    "# Access file paths and labels\n",
    "file_paths = mel_dataset.file_paths\n",
    "labels = mel_dataset.labels\n",
    "\n",
    "# Split dataset using stratification\n",
    "train_file_paths, temp_file_paths, train_labels, temp_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_file_paths, test_file_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_file_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "# Create Datasets for each split\n",
    "train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "test_indices = [file_paths.index(path) for path in test_file_paths]\n",
    "\n",
    "train_dataset = Subset(mel_dataset, train_indices)\n",
    "val_dataset = Subset(mel_dataset, val_indices)\n",
    "test_dataset = Subset(mel_dataset, test_indices)\n",
    "\n",
    "# Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "# Get train dataset labels\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Get train dataset label count for each class\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = 1.0 / class_counts\n",
    "weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "# Model\n",
    "\n",
    "# Define the model with dynamic num_filters\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "# Set fixed random number of seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plain training loop without early stopping\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in train_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item()\n",
    "        train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "    # Evaluate the CNN model on the validation set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_spectrogram, label in test_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "            all_probs.append(probabilities)\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "            all_labels_per_class.extend(label.cpu().numpy())\n",
    "            all_probs_per_class.extend(output.cpu().numpy())\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "\n",
    "# Save this model\n",
    "torch.save(model.state_dict(), \"models/resnet_model_v1_weighted.pth\")\n",
    "# Evaluate the model on the test set"
   ],
   "id": "4f2402bb877687d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "test_model = ResNetRNN(input_size=512, hidden_size=512, num_layers=best_params['num_layers'], num_classes=8).to(DEVICE)\n",
    "\n",
    "test_model = test_model.to(\"mps\")\n",
    "\n",
    "# Load the model weights\n",
    "test_model.load_state_dict(torch.load(\"models/resnet_model_v1_weighted.pth\", weights_only=True))\n",
    "\n",
    "# Create a DataLoader instance for the test set\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = test_model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        all_labels_per_class.extend(label.cpu().numpy())\n",
    "        all_probs_per_class.extend(output.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "        test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")"
   ],
   "id": "f6a4f36317f2d0d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the confusion matrix with annotations on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, y_pred)\n",
    "\n",
    "# Plot the confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()\n",
    "# Now plot the normalized confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_norm, cmap=\"Blues\", annot=True, fmt=\".2f\", xticklabels=mel_dataset.genres,\n",
    "            yticklabels=mel_dataset.genres)\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ],
   "id": "51b1036ae59ce62c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get the true positive rate and false positive rate\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(8):\n",
    "    fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc[i] = roc_auc_score((all_labels == i).astype(int), all_probs[:, i])\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(8):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{mel_dataset.genres[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "da68494baf3429a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Worse with pop but better with f.ex. rap, metal, hiphop",
   "id": "4272728640ae7aa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ResNet18 + LSTM Hybrid Model\n",
    "# Create a class for hybrid model (ResNet18 + LSTM), import model from Torchvision, models. Input size of the model should be the same as spectogram shape (1, 96, 1376)    \n",
    "\n",
    "class ResNetLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ResNetLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for ResNet18 + LSTM hybrid.\n",
    "    \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (Batch, Channels, Height, Width).\n",
    "    \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (Batch, num_classes).\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure input is 4D (Batch, Channels, Height, Width)\n",
    "        if x.dim() == 3:  # If input is (Batch, Height, Width)\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "                \n",
    "        # Pass input through ResNet18 layers\n",
    "        features = self.resnet.conv1(x)\n",
    "        features = self.resnet.bn1(features)\n",
    "        features = self.resnet.relu(features)\n",
    "        features = self.resnet.maxpool(features)\n",
    "    \n",
    "        features = self.resnet.layer1(features)\n",
    "        features = self.resnet.layer2(features)\n",
    "        features = self.resnet.layer3(features)\n",
    "        features = self.resnet.layer4(features)\n",
    "            \n",
    "        # Shape after ResNet: (Batch, Channels, Height, Width)\n",
    "        batch_size, channels, height, width = features.size()\n",
    "    \n",
    "        # Reshape features for LSTM: (Batch, Time, Features)\n",
    "        # Time = Width, Features = Channels * Height\n",
    "        rnn_input = features.permute(0, 3, 1, 2).reshape(batch_size, width, channels * height)\n",
    "            \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(rnn_input)  # LSTM outputs all timesteps\n",
    "            \n",
    "        # Use the last timestep's output\n",
    "        final_output = self.fc(lstm_out[:, -1, :])  # Shape: (Batch, num_classes)\n",
    "    \n",
    "        return final_output\n",
    "\n",
    "        "
   ],
   "id": "dc6d73ad17fc6bbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "input_size = 512 * 3# Channels * Height from ResNet output\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 8\n",
    "batch_size = 32\n",
    "\n",
    "model = ResNetLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.randn(batch_size, 1, 96, 1376) # Batch size 2, 1 channel, 96x1376 input\n",
    "output = model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be (Batch, num_classes)\n"
   ],
   "id": "29aa0a0fa633f55f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "e94f365ae46f049b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pop and hiphop are overwhealming rest of the classes because of the dataset imbalance\n",
    "# Let's try to undersample the dataset genres: pop and hiphop and train again.\n",
    "# Change the max_samples_dict to undersample pop and hiphop genres in the objective and train the model again.\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Layers of the RNN to optimize\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "\n",
    "    # Create a MelSpectogramDataset instance\n",
    "    \"\"\"\n",
    "    Training the model with stratified split, apply weighted random sampler with Optuna.\n",
    "    \"\"\"\n",
    "    # Create dataset instance\n",
    "\n",
    "    mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "    # Split the dataset into train, validation and test with 90-5-5 split\n",
    "\n",
    "    # Access file paths and labels\n",
    "    file_paths = mel_dataset.file_paths\n",
    "    labels = mel_dataset.labels\n",
    "\n",
    "    # Split dataset using stratification to train - validate\n",
    "    train_file_paths, val_file_paths, train_labels, val_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Create Datasets for each split\n",
    "    train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "    val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "\n",
    "    train_dataset = Subset(mel_dataset, train_indices)\n",
    "    val_dataset = Subset(mel_dataset, val_indices)\n",
    "\n",
    "    # Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "    # Get train dataset labels\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "    # Get train dataset label count for each class\n",
    "    class_counts = np.bincount(train_labels)\n",
    "\n",
    "    # Calculate the class weights\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "    # Create a WeightedRandomSampler\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Set fixed random number of seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Make use of GPU if available or MPS (Apple) if one is available\n",
    "    device = (\n",
    "        \"mps\"\n",
    "        if torch.backends.mps.is_built()\n",
    "        else \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "    # Define the model with dynamic num_filters\n",
    "    model = ResNetLSTM(input_size=512 * 3, hidden_size=512, num_layers=num_layers, num_classes=8).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 4\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        for mel_spectrogram, label in train_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "        # Evaluate the CNN model\n",
    "        model.eval()\n",
    "        all_labels, all_probs = [], []\n",
    "        test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        # Use Early Stopping\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, label in test_progress:\n",
    "                mel_spectrogram, label = mel_spectrogram.to(device).float(), label.to(device)\n",
    "                output = model(mel_spectrogram.unsqueeze(1))\n",
    "                probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "                all_probs.append(probabilities)\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "        # Concatenate all predictions and true labels\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = calculate_metrics(all_labels, all_probs, num_classes=8)[\"F1 Score\"]\n",
    "\n",
    "        trial.report(f1, epoch)\n",
    "\n",
    "        # Prune if needed based on the reported F1 score\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Optionally, print the F1 score for each epoch\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the F1 score after the training loop\n",
    "    return f1\n"
   ],
   "id": "e43b222514d516c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna study\n",
    "study_name = \"resnet_lstm_hybrid_optimization_v1_weighted_stratified\"\n",
    "# Create a file to save study name and my comments on used dataset and hyperparameters\n",
    "study_comment = \"Full dataset. Optimizing learning rate, batch size. No cross-validation used. 90-10 split for training and validation. Stratified split used. WeightedRandomSampler used for training. LSTM+ResNet model.\"\n",
    "study_comment_file = \"study_comment.csv\"\n",
    "\n",
    "# Write new line with study name and comment\n",
    "with open(study_comment_file, \"a\") as f:\n",
    "    f.write(f\"\\n{study_name},{study_comment}\")\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///db.sqlite3\", study_name=study_name,\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials=15)"
   ],
   "id": "9861dddcfdaaa399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best trial\n",
    "# print(\"Best trial:\")\n",
    "# print(f\"F1 Score: {study.best_trial.value}\")\n",
    "# print(f\"Params: {study.best_trial.params}\")\n",
    "# \n",
    "# best_params = study.best_trial.params\n",
    "model = ResNetLSTM(input_size=512 * 3, hidden_size=512, num_layers=best_params['num_layers'], num_classes=8).to(DEVICE)\n",
    "# Prepare data\n",
    "\n",
    "mel_dataset = MelSpectogramDataset(data_path='spectograms_data')\n",
    "\n",
    "# Split the dataset into train, validation and test with 90-5-5 split\n",
    "# Access file paths and labels\n",
    "file_paths = mel_dataset.file_paths\n",
    "labels = mel_dataset.labels\n",
    "\n",
    "# Split dataset using stratification\n",
    "train_file_paths, temp_file_paths, train_labels, temp_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_file_paths, test_file_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_file_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "# Create Datasets for each split\n",
    "train_indices = [file_paths.index(path) for path in train_file_paths]\n",
    "val_indices = [file_paths.index(path) for path in val_file_paths]\n",
    "test_indices = [file_paths.index(path) for path in test_file_paths]\n",
    "\n",
    "train_dataset = Subset(mel_dataset, train_indices)\n",
    "val_dataset = Subset(mel_dataset, val_indices)\n",
    "test_dataset = Subset(mel_dataset, test_indices)\n",
    "\n",
    "# Handle imabalanced dataset with WeightedRandomSampler\n",
    "\n",
    "# Get train dataset labels\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Get train dataset label count for each class\n",
    "class_counts = np.bincount(train_labels)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = 1.0 / class_counts\n",
    "weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])"
   ],
   "id": "d1db8e2d1d225ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model\n",
    "\n",
    "# Define the model with dynamic num_filters\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "# Set fixed random number of seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plain training loop without early stopping\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_progress = tqdm.tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in train_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item()\n",
    "        train_progress.set_postfix({\"Loss\": f\"{running_loss / (train_progress.n + 1):.4f}\"})\n",
    "\n",
    "    # Evaluate the CNN model on the validation set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(val_loader, desc=f\"Testing Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_spectrogram, label in test_progress:\n",
    "            mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "            output = model(mel_spectrogram.unsqueeze(1))\n",
    "            probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "            all_probs.append(probabilities)\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "            all_labels_per_class.extend(label.cpu().numpy())\n",
    "            all_probs_per_class.extend(output.cpu().numpy())\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "\n",
    "# Save this model\n",
    "torch.save(model.state_dict(), \"models/resnet_lstm_model_v1_weighted.pth\")\n",
    "# Params: {'learning_rate': 1.8529931417413163e-05, 'batch_size': 16, 'num_layers': 2}"
   ],
   "id": "7ca436ec54ca4d19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training history with amtplotlib, use LateX font for better visualization\n",
    "# Load the model weights\n",
    "f1_values = [0.3862807751455983, 0.38883032407612794, 0.42452064798923006, 0.38011824897377877, 0.3984572420100985, 0.4114444715334195, 0.4179873479786368, 0.4050534271769628]\n",
    "\n",
    "# Use LaTeX font\n",
    "plt.rc(\"text\", usetex=True)\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(f1_values, label=\"F1 Score\", marker=\"o\")\n",
    "plt.title(\"Training History\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "a0cdb9f413f639b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "best_params = {'learning_rate': 1.8529931417413163e-05, 'batch_size': 16, 'num_layers': 2}\n",
    "\n",
    "test_model = ResNetLSTM(input_size=512*3, hidden_size=512, num_layers=best_params['num_layers'], num_classes=8).to(DEVICE)\n",
    "\n",
    "test_model = test_model.to(\"mps\")\n",
    "\n",
    "# Load the model weights\n",
    "test_model.load_state_dict(torch.load(\"models/resnet_lstm_model_v1_weighted.pth\", weights_only=True))\n",
    "\n",
    "# Create a DataLoader instance for the test set\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    all_labels_per_class, all_probs_per_class = [], []\n",
    "    test_progress = tqdm.tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = test_model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        all_labels_per_class.extend(label.cpu().numpy())\n",
    "        all_probs_per_class.extend(output.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "        test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    all_labels_per_class = np.array(all_labels_per_class)\n",
    "    all_probs_per_class = np.array(all_probs_per_class)\n",
    "    y_pred = all_probs_per_class.argmax(axis=1)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(\"PER CLASS METRICS\")\n",
    "    per_class_metrics(all_labels, y_pred, num_classes=8)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")\n",
    "# Plot the confusion matrix with annotations on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, y_pred)\n",
    "\n",
    "# Plot the confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()\n",
    "# Now plot the normalized confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_norm, cmap=\"Blues\", annot=True, fmt=\".2f\", xticklabels=mel_dataset.genres,\n",
    "            yticklabels=mel_dataset.genres)\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get the true positive rate and false positive rate\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(8):\n",
    "    fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc[i] = roc_auc_score((all_labels == i).astype(int), all_probs[:, i])\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(8):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{mel_dataset.genres[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b5c245fbf2eeb50f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d145a119fdfc39d0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
