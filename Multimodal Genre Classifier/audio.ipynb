{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, roc_curve, classification_report\n",
    "import seaborn as sns\n",
    "from utils.audio_utils import MelSpectogramDataset\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2ad0dd8ae1883b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred_probs, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate AUC, precision, recall, and F1 score for multiclass classification.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred_probs (array-like): Predicted probabilities or logits.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing AUC, precision, recall, and F1 scores.\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to class predictions\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_pred_probs, multi_class=\"ovr\", average=\"macro\"),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }\n",
    "    \n",
    "    print(classification_report(y_true, y_pred, target_names=[f\"Class {i}\" for i in range(num_classes)]))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def per_class_metrics(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Prints classification metrics for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "    \"\"\"\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        zero_division=1,\n",
    "        target_names=[f\"Class {i}\" for i in range(num_classes)]\n",
    "    )\n",
    "    print(report)"
   ],
   "id": "73b88df09fad0325",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model on the test set\n",
    "model = models.resnet18()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "model.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "model.fc = nn.Linear(512, 8)\n",
    "#summary(model, (1, 96, 1024))\n",
    "\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/resnet_model_v1_weighted_alphabetical_normalize_fma.pt\", weights_only=True))"
   ],
   "id": "cb7d8fd9659955ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "69b3bb4ea40d1f05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mel_dataset = MelSpectogramDataset(data_path='mel_spectrogram')\n",
    "dataset_loader = DataLoader(mel_dataset, batch_size=64)"
   ],
   "id": "712fa56d29f21a8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    all_labels, all_probs = [], []\n",
    "    test_progress = tqdm(dataset_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_probs, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")"
   ],
   "id": "49412c0d8df14514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the true positive rate and false positive rate\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(8):\n",
    "    fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "    roc_auc[i] = roc_auc_score((all_labels == i).astype(int), all_probs[:, i])\n",
    "\n",
    "# Save data to plot the ROC curve\n",
    "\n",
    "np.save(\"fpr_resnet.npy\", fpr)\n",
    "np.save(\"tpr_resnet.npy\", tpr)\n",
    "np.save(\"roc_auc_resnet.npy\", roc_auc)\n",
    "\n",
    "# Save label\n",
    "np.save(\"labels_resnet.npy\", mel_dataset.genres)\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "for i in range(8):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{mel_dataset.genres[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "# Make background plain white\n",
    "plt.style.use('fast')\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.savefig(\"auc_roc_resnet_3.eps\", dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "fcc7ba1d34896330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_probs[291]",
   "id": "5a049db36ae2089f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_labels[0]",
   "id": "e84caf278f62e268",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_pred[121]",
   "id": "57b532df87b27be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the confusion matrix with annotations on test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_probs.argmax(axis=1))\n",
    "\n",
    "# Plot the confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xticks(range(8), mel_dataset.genres, rotation=45)\n",
    "plt.yticks(range(8), mel_dataset.genres)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.show()"
   ],
   "id": "da7c373ba6e51e17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = [\"white\", \"#455681\"]  # White to #455681 gradient\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_white_to_blue\", colors)"
   ],
   "id": "f5ef758a76134e50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize the confusion matrix# \n",
    "conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_norm, cmap=custom_cmap, annot=True, fmt=\".2f\", xticklabels=mel_dataset.genres,\n",
    "            yticklabels=mel_dataset.genres, vmax=1.0)\n",
    "plt.xlabel(\"Predicted\", fontdict={\"fontsize\": 12})\n",
    "plt.ylabel(\"True\", fontdict={\"fontsize\": 12})\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_normalized_resnet_fma.eps\", dpi=300)\n",
    "plt.show()"
   ],
   "id": "d71d4f5f05e7423a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Draw AUC-ROC curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "id": "e6765f820c0bfaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data from csv for audio\n",
    "\n",
    "from utils.audio_utils import MelSpectogramDataset\n",
    "\n",
    "mel_dataset = MelSpectogramDataset(data_path='mel_spectrogram')\n",
    "dataset_loader = DataLoader(mel_dataset, batch_size=32)\n",
    "# Evaluate the model on the test set\n",
    "audio_model = models.resnet18()\n",
    "audio_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=(1, 2), bias=True)\n",
    "audio_model.maxpool = nn.MaxPool2d((2, 3), stride=(1, 2))\n",
    "audio_model.fc = nn.Linear(512, 8)\n",
    "\n",
    "audio_model = audio_model.to(\"mps\")\n",
    "\n",
    "audio_model.load_state_dict(torch.load(\"models/resnet_model_v1_weighted_alphabetical.pt\", weights_only=True))\n",
    "# Get number of samples in test_loader\n",
    "\n",
    "num_samples = 0\n",
    "for mel_spectrogram, label in dataset_loader:\n",
    "    num_samples += mel_spectrogram.size(0)\n",
    "print(num_samples)"
   ],
   "id": "d33a0a9c23145fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get predictions from the audio model\n",
    "# Get predictions from the audio model\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    audio_preds, audio_labels = [], []\n",
    "    test_progress = tqdm(dataset_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    for mel_spectrogram, label in test_progress:\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = audio_model(mel_spectrogram.unsqueeze(1))\n",
    "        probabilities = nn.Softmax(dim=1)(output).cpu().numpy()\n",
    "        audio_preds.append(probabilities)\n",
    "        audio_labels.append(label.cpu().numpy())\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "        test_progress.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    audio_labels = np.concatenate(audio_labels)\n",
    "    audio_preds = np.concatenate(audio_preds)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(audio_labels, audio_preds, num_classes=8)\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}% | Metrics: {metrics}\")"
   ],
   "id": "dbcca7af9e6386e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extract features from the audio model on the multi-modal dataset",
   "id": "ffb0aa70339ba3f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.audio_utils import MelSpectogramDataset\n",
    "\n",
    "mel_dataset = MelSpectogramDataset(data_path='mel_spectrogram')\n",
    "multimodal_dataset_loader = DataLoader(mel_dataset, batch_size=64)"
   ],
   "id": "77ff6ba55af65855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check how many samples are in the dataset\n",
    "num_samples = 0\n",
    "for mel_spectrogram, label in multimodal_dataset_loader:\n",
    "    num_samples += mel_spectrogram.size(0)\n",
    "print(num_samples)"
   ],
   "id": "6076d959cb70d836",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract features from the audio model\n",
    "model = models.resnet18()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "model.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "model.fc = nn.Linear(512, 8)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/resnet_model_v1_weighted_alphabetical_normalize_fma.pt\", weights_only=True))\n",
    "model = model.to(\"mps\")\n",
    "\n",
    "# remove the last layer\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()\n",
    "\n",
    "# extract features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel_spectrogram, label in tqdm(multimodal_dataset_loader, desc=\"Extracting Features\", leave=False):\n",
    "        mel_spectrogram, label = mel_spectrogram.to(\"mps\").float(), label.to(\"mps\")\n",
    "        output = model(mel_spectrogram.unsqueeze(1))\n",
    "        features.append(output.cpu().numpy())\n",
    "        labels.append(label.cpu().numpy())"
   ],
   "id": "4fc323d11a446587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features[0].shape",
   "id": "6acac87ec0b60682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features_concat = np.concatenate(features)\n",
    "labels_concat = np.concatenate(labels)"
   ],
   "id": "554ce627a51fd6b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features_concat.shape",
   "id": "4edf4a683c04f109",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "labels_concat.shape",
   "id": "846c1817f3809e1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Resize features to 2D\n",
    "features_2d = features_concat.reshape(features_concat.shape[0], -1)\n",
    "features_2d.shape"
   ],
   "id": "cb738775e696c2fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show example feature with label\n",
    "example_idx = 0\n",
    "print(f\"Label: {labels_concat[example_idx]}\")\n",
    "print(features_2d[example_idx].shape)"
   ],
   "id": "4259a2e06c92cf23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features_2d",
   "id": "65e34c3713a7a8fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(labels_concat)",
   "id": "7ad53185123b1741",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the features and labels\n",
    "np.save(\"features/features_audio.npy\", features_2d)\n",
    "np.save(\"features/labels_audio.npy\", labels_concat)"
   ],
   "id": "fd21703d7b962ba0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
